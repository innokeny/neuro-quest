{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be308d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee758de",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9933d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "DATASET_PATH = Path(\"FIREBALL/filtered\")\n",
    "\n",
    "MODEL_SAVE_DIR = Path(\"../models/core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280fe739",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238baf0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8aaaa5",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b40f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATASET_PATH.parent.exists():\n",
    "    !git lfs install && git clone https://huggingface.co/datasets/lara-martin/FIREBALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a44c6",
   "metadata": {},
   "source": [
    "from [dataset info](https://huggingface.co/datasets/lara-martin/FIREBALL)\n",
    "```\n",
    "{\n",
    "    \"speaker_id\": The anonymized user ID of the user who sent the commands in the triple. \n",
    "    \"before_utterances\": A list of strings corresponding to the \"preceding\" utterances in the triple.\n",
    "    \"combat_state_before\": A list of normalized actor states (see below) for each actor in the combat instance at the instant before the command was run.\n",
    "    \"current_actor\": (nullable) The normalized actor state of the actor whose turn it currently is.\n",
    "    \"commands_norm\": A list of strings corresponding to the \"commands\" portion of the triple.\n",
    "    \"automation_results\": A mechanically generated list of strings representing the results of running the action in the Avrae engine.\n",
    "    \"caster_after\": The normalized actor state of the actor who ran the action(s), which may or may not be the current actor.\n",
    "    \"targets_after\": A list of normalized actor states for each actor who was targeted by the action.\n",
    "    \"combat_state_after\": A list of normalized actor states for each actor in the combat instance at the instant after the command was run.\n",
    "    \"after_utterances\": A list of strings corresponding to the \"following\" utterances in the triple.\n",
    "    \"utterance_history\": The last 5 messages in the chat history before the command was run.\n",
    "    \"before_idxs\": A list of integers corresponding to the index of the \"message\" events containing the \"preceding\" utterances in the raw event file.\n",
    "    \"before_state_idx\": The index of the \"combat_state_update\" event in the raw event file that was used to derive \"combat_state_before\".\n",
    "    \"command_idxs\": The indexes of the \"command\" events corresponding to the \"commands_norm\" key.\n",
    "    \"after_state_idx\": The index of the \"combat_state_update\" event corresponding to the \"combat_state_after\" key.\n",
    "    \"after_idxs\": The indexes of the \"message\" events corresponding to the \"after_utterances\" key.\n",
    "    \"embed_idxs\": (nullable, same length as \"automation_results\") The indexes of \"message\" events corresponding to rich results shown to players on Discord for each result in the \"automation_results\" key.\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ef245",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "652570b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_filtrate(s: str) -> str:\n",
    "    return s.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"*\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8213cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obj: dict | pd.Series) -> tuple[str, str]:\n",
    "    before_utterances = obj.get(\"before_utterances\", [])\n",
    "    automation_results =  obj.get(\"automation_results\", [])\n",
    "    after_utterances = obj.get(\"after_utterances\", [])\n",
    "    return f'''\n",
    "[CONTEXT]: {string_filtrate(' '.join(before_utterances))}\n",
    "[COMMAND]: {string_filtrate(' '.join(automation_results))}\n",
    "''', string_filtrate(' '.join(after_utterances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c6bb033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1471it [00:06, 226.21it/s]\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "\n",
    "for file in tqdm(DATASET_PATH.iterdir()):\n",
    "    with jsonlines.open(file) as reader:\n",
    "        for obj in reader:\n",
    "            prompt, response = preprocess(obj)\n",
    "            if prompt and response:\n",
    "                samples.append({\"prompt\": prompt, \"response\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0a93b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(samples).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dfb30414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response'],\n",
       "        num_rows: 39031\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response'],\n",
       "        num_rows: 4337\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1bd01b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcad3b5",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ba151c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",\n",
    "    pad_token=\"[EOT]\" # end of text\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": \"[EXTRA_0]\",\n",
    "    \"eos_token\": \"[EOT]\",\n",
    "    \"bos_token\": \"[EOT]\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dccbb2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151671, 1024)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={'': torch.cuda.current_device()},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c2e996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_tokenize(examples):\n",
    "    texts = [\n",
    "        f\"[im_start]system\\nТы Dungeon Master[im_end]\\n\"\n",
    "        f\"[im_start]user\\n{str(prompt)}[im_end]\\n\"\n",
    "        f\"[im_start]assistant\\n{str(response)}[im_end]\"\n",
    "        for prompt, response in zip(examples['prompt'], examples['response'])\n",
    "    ]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    # Убираем requires_grad и преобразуем в списки\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"].tolist(),\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"].tolist(),\n",
    "        \"labels\": tokenized[\"input_ids\"].tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3447b6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3c00c5ec8b41e69676ec91f2bfd700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting and tokenizing:   0%|          | 0/39031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68735b98f9046b086fe3c47996ef049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting and tokenizing:   0%|          | 0/4337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    format_and_tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\", \"response\"],\n",
    "    desc=\"Formatting and tokenizing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5534fa1",
   "metadata": {},
   "source": [
    "##  LoRa config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5b598c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,146,880 || all params: 596,925,440 || trainable%: 0.1921\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Уменьшено с 16\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Только ключевые модули\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3179de",
   "metadata": {},
   "source": [
    "## Train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "13084840",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_DIR,\n",
    "    per_device_train_batch_size=4,  # Увеличен размер батча (если позволяет память GPU)\n",
    "    gradient_accumulation_steps=2,  # Увеличена аккумуляция градиентов\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_bnb_8bit\",  # Более эффективный оптимизатор\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=False,  # Отключено для ускорения\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=True,\n",
    "    label_names=[\"labels\"],\n",
    "    max_grad_norm=0.3,\n",
    "    dataloader_num_workers=2,  # Параллельная загрузка данных\n",
    "    torch_compile=False  # Компиляция графа вычислений\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8a8a65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "27df1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a41bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2712' max='4879' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2712/4879 49:52 < 39:52, 0.91 it/s, Epoch 0.56/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.114800</td>\n",
       "      <td>2.177814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.127200</td>\n",
       "      <td>2.121910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.073800</td>\n",
       "      <td>2.085763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.028000</td>\n",
       "      <td>2.059729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.942600</td>\n",
       "      <td>2.037523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/user/education/neuro-quest/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/user/education/neuro-quest/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/user/education/neuro-quest/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/user/education/neuro-quest/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/user/education/neuro-quest/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b359f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
